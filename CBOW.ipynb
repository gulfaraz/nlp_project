{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def read_files():\n",
    "    datastore={}\n",
    "    visual_feat_mapping={}\n",
    "    img_feat_data={}\n",
    "    img_features = np.asarray(h5py.File('IR_image_features.h5', 'r')['img_features'])\n",
    "\n",
    "    with open('IR_img_features2id.json', 'r') as f:\n",
    "        visual_feat_mapping = json.load(f)['IR_imgid2id']\n",
    "\n",
    "    with open(\"IR_train_easy.json\") as tweetfile:\n",
    "        datastore = json.load(tweetfile)\n",
    "    for key in visual_feat_mapping.keys():\n",
    "        h5_id = visual_feat_mapping[key]\n",
    "        img_feat = img_features[h5_id]\n",
    "        img_feat_data[key]=img_feat    \n",
    "        \n",
    "    return datastore,visual_feat_mapping,img_feat_data   \n",
    "def create_vocab(datastore):\n",
    "    w2i={}\n",
    "    i2w={}\n",
    "    counter=Counter()\n",
    "    di={}\n",
    "    for i in range(0,40000):\n",
    "        dialogs=datastore[str(i)]['dialog']\n",
    "        for dialog in dialogs:\n",
    "            for line in dialog:\n",
    "                tokens=word_tokenize(line)\n",
    "                for token in tokens:\n",
    "                    counter[token]+=1\n",
    "    index=0      \n",
    "    counter=counter.most_common(500)\n",
    "    for key in counter:\n",
    "        w2i[key[0]]=index\n",
    "        i2w[index]=key[0]\n",
    "        index+=1\n",
    "    w2i[\"invalid\"]=index\n",
    "    i2w[index]=\"invalid\"\n",
    "    return w2i,i2w    \n",
    "\n",
    "datastore,visual_feat_mapping,img_feat_data=read_files()\n",
    "w2i,i2w=create_vocab(datastore)\n",
    "print(\"Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Variable containing:\n",
      " 470.8598\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1\n",
      "Variable containing:\n",
      " 465.5178\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2\n",
      "Variable containing:\n",
      " 457.4916\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3\n",
      "Variable containing:\n",
      " 452.7193\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4\n",
      "Variable containing:\n",
      " 444.7437\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 5\n",
      "Variable containing:\n",
      " 437.3271\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 6\n",
      "Variable containing:\n",
      " 428.6427\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 7\n",
      "Variable containing:\n",
      " 417.2307\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 8\n",
      "Variable containing:\n",
      " 412.3379\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 9\n",
      "Variable containing:\n",
      " 410.6841\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 10\n",
      "Variable containing:\n",
      " 408.2955\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 11\n",
      "Variable containing:\n",
      " 410.3102\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 12\n",
      "Variable containing:\n",
      " 412.9070\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 13\n",
      "Variable containing:\n",
      " 415.1518\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 14\n",
      "Variable containing:\n",
      " 416.7797\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 15\n",
      "Variable containing:\n",
      " 415.7545\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 16\n",
      "Variable containing:\n",
      " 413.9796\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 17\n",
      "Variable containing:\n",
      " 414.9532\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 18\n",
      "Variable containing:\n",
      " 414.9382\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 19\n",
      "Variable containing:\n",
      " 415.0209\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Input Done\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.FloatTensor\n",
    "learning_rate =0.0009\n",
    "def create_nueral_net():\n",
    "    errors=[]\n",
    "    w_in = Variable(torch.randn(501,1).type(dtype),requires_grad=True)\n",
    "    w_out = Variable(torch.randn(1,2048).type(dtype),requires_grad=True)\n",
    "    loss =nn.CrossEntropyLoss()\n",
    "    m = nn.Softmax()\n",
    "    #text_input=torch.Tensor(len(w2i))\n",
    "    #img_input=torch.Tensor(2048)\n",
    "    for epochs in range(0,30):\n",
    "        error=0 \n",
    "        print(\"epoch \"+str(epochs))\n",
    "        start=0\n",
    "        end=400\n",
    "        for loop in range(0,1):\n",
    "            for i in range(start,end):  \n",
    "                #print(i)\n",
    "                dialogs=datastore[str(i)]['dialog']\n",
    "                text_inputs=torch.Tensor(len(dialogs),len(w2i),1)\n",
    "                index=0\n",
    "                image_index=datastore[str(i)]['target']\n",
    "                for dialog in dialogs:\n",
    "                    for line in dialog:\n",
    "                        text_input=torch.zeros(len(w2i),1)\n",
    "                        tokens=word_tokenize(line)\n",
    "                        for token in tokens:\n",
    "                            if token in w2i.keys():\n",
    "                               text_input[w2i[token]]=text_input[w2i[token]]+1.0\n",
    "                            else:\n",
    "                               text_input[w2i[\"invalid\"]]=text_input[w2i[\"invalid\"]]+1.0   \n",
    "                        text_inputs[index]=text_input\n",
    "                        index=index+1\n",
    "                images_arr=datastore[str(i)]['img_list']\n",
    "                image_data=torch.Tensor(2048,1)\n",
    "                images=torch.Tensor(len(images_arr),2048,1)\n",
    "                for k in range(0,len(images_arr)):\n",
    "                    image_data=img_feat_data[str(images_arr[k])]\n",
    "                    images[k]=torch.from_numpy(image_data)\n",
    "                index=0\n",
    "           \n",
    "                y_target =Variable(torch.zeros(1).type(torch.LongTensor), requires_grad=False)\n",
    "                y_predict =Variable(torch.zeros(1,10), requires_grad=False)\n",
    "                for k in range(0,len(images_arr)):\n",
    "                    \n",
    "                    input_data=Variable(torch.zeros(2048,501), requires_grad=False)\n",
    "                    hidden_layer_sum=Variable(torch.zeros(2048,1), requires_grad=True)\n",
    "                    hidden_layer_data=Variable(torch.zeros(2048,1), requires_grad=True)\n",
    "            \n",
    "                    if(k==image_index):\n",
    "                       y_target.data[0]=k\n",
    "                    for j in range(0,len(text_inputs)):\n",
    "                        temp=text_inputs[j].transpose(0,1)\n",
    "                        temp2=images[k].mm(temp)\n",
    "                        input_data.data=temp2\n",
    "                        hidden_layer_data=input_data.mm(w_in)\n",
    "                        hidden_layer_sum= hidden_layer_sum+hidden_layer_data\n",
    "                    hidden_layer_sum=torch.div(hidden_layer_sum, len(text_inputs))\n",
    "                    temp_output=w_out.mm(hidden_layer_sum)\n",
    "                    y_predict[0,k]=temp_output\n",
    "                   \n",
    "                #print(y_predict.type)\n",
    "                #print(y_target.type)\n",
    "                y_predict=m(y_predict)\n",
    "                loss_for_image = loss( y_predict,y_target)\n",
    "                error=error+loss_for_image\n",
    "                loss_for_image.backward()\n",
    "                w_in.data -= learning_rate * w_in.grad.data\n",
    "                w_out.data -= learning_rate * w_out.grad.data\n",
    "                    #print(error)   \n",
    "                w_in.grad.data.zero_()\n",
    "                w_in.grad.data.zero_()\n",
    "            \n",
    "            #start=start+5000\n",
    "            #end=end+5000\n",
    "            print(error)  \n",
    "            errors.append(error)\n",
    "            \n",
    "\n",
    "    return w_in,w_out,errors        \n",
    "\n",
    "w_in,w_out,errors=create_nueral_net()\n",
    "\n",
    "print(\"Input Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax()\n",
    "def preditct_values(w_in,w_out):\n",
    "    count=0\n",
    "    for i in range(0,100):  \n",
    "        #print(i)\n",
    "        dialogs=datastore[str(i)]['dialog']\n",
    "        text_inputs=torch.Tensor(len(dialogs),len(w2i),1)\n",
    "        index=0\n",
    "        image_index=datastore[str(i)]['target']\n",
    "        for dialog in dialogs:\n",
    "            for line in dialog:\n",
    "                text_input=torch.zeros(len(w2i),1)\n",
    "                tokens=word_tokenize(line)\n",
    "                for token in tokens:\n",
    "                    if token in w2i.keys():\n",
    "                       text_input[w2i[token]]=text_input[w2i[token]]+1.0\n",
    "                    else:\n",
    "                       text_input[w2i[\"invalid\"]]=text_input[w2i[\"invalid\"]]+1.0   \n",
    "                text_inputs[index]=text_input\n",
    "                index=index+1\n",
    "        images_arr=datastore[str(i)]['img_list']\n",
    "        image_data=torch.Tensor(2048,1)\n",
    "        images=torch.Tensor(len(images_arr),2048,1)\n",
    "        for k in range(0,len(images_arr)):\n",
    "            image_data=img_feat_data[str(images_arr[k])]\n",
    "            images[k]=torch.from_numpy(image_data)\n",
    "        index=0\n",
    "   \n",
    "        y_target =Variable(torch.zeros(1).type(torch.LongTensor), requires_grad=False)\n",
    "        y_predict =Variable(torch.zeros(1,10), requires_grad=False)\n",
    "        for k in range(0,len(images_arr)):\n",
    "            \n",
    "            input_data=Variable(torch.zeros(2048,501), requires_grad=False)\n",
    "            hidden_layer_sum=Variable(torch.zeros(2048,1), requires_grad=True)\n",
    "            hidden_layer_data=Variable(torch.zeros(2048,1), requires_grad=True)\n",
    "    \n",
    "            if(k==image_index):\n",
    "               y_target.data[0]=k\n",
    "            for j in range(0,len(text_inputs)):\n",
    "                temp=text_inputs[j].transpose(0,1)\n",
    "                temp2=images[k].mm(temp)\n",
    "                input_data.data=temp2\n",
    "                hidden_layer_data=input_data.mm(w_in)\n",
    "                hidden_layer_sum= hidden_layer_sum+hidden_layer_data\n",
    "            hidden_layer_sum=torch.div(hidden_layer_sum, len(text_inputs))\n",
    "            temp_output=w_out.mm(hidden_layer_sum)\n",
    "            y_predict[0,k]=temp_output\n",
    "        y_predict=m(y_predict)\n",
    "        max1=y_predict[0,0].data[0]\n",
    "        #print(max1)\n",
    "        pos=1\n",
    "        for l in range(1,10):\n",
    "            if(y_predict[0,l].data[0]>max1):\n",
    "                max1=y_predict[0,l].data[0]\n",
    "                pos=l\n",
    "        if(pos==y_target.data[0]):\n",
    "            count=count+1\n",
    "    print(count)  \n",
    "                \n",
    "preditct_values(w_in,w_out)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c53daf4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "x=np.zeros(len(errors))\n",
    "y=np.zeros(len(errors))\n",
    "for i in range(0,len(errors)):\n",
    "    x[i]=i\n",
    "    y[i]=errors[i].data[0]\n",
    "plt.scatter(x,y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2\n",
      " 4\n",
      " 0\n",
      "[torch.LongTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
